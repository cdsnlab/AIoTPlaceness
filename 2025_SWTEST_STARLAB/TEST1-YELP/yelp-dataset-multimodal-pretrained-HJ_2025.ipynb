{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bcbe309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"   # 또는 \":16:8\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"      # (HF 토크나이저 스레딩 고정용)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da86a308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa22a877610>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 42  # 딱 하나만 바꿔서 실험 구분\n",
    "\n",
    "# (1) 파이썬/넘파이/파이토치 시드\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)   # multi-GPU 대비\n",
    "\n",
    "# (2) 완전 결정론 모드 (성능 약간 하락 가능)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# (3) cuDNN/TF32 비활성화\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    # 각 worker별 고유 시드 부여\n",
    "    worker_seed = (SEED + worker_id) % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe295821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD = /disk1/jupyter/placeness/SWTEST-2024/TEST1-YELP\n"
     ]
    }
   ],
   "source": [
    "os.listdir('yelp-dataset')\n",
    "print(\"CWD =\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58657d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf = pd.read_json('yelp_dataset_10000.json')\n",
    "label_mapping = {label: idx for idx, label in enumerate(fdf['label'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d04f2223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 6)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdf.to_csv('yelp_dataset_10000.csv', index=False)\n",
    "fdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ffeeed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame (replace this with your actual DataFrame)\n",
    "df = pd.DataFrame({'photo_path': fdf['photo_path'].tolist(),\n",
    "                   'caption': fdf['caption'], \n",
    "                   'label': fdf['labelidx']})\n",
    "\n",
    "# Split data into Test (20%) and Temp (80%) sets\n",
    "temp_df, test_df = train_test_split(df, test_size=0.2, random_state=SEED, stratify=df['label'])\n",
    "\n",
    "# Further split Temp into Train (60% of original) and Validation (20% of original)\n",
    "train_df, val_df = train_test_split(temp_df, test_size=0.25, random_state=SEED, stratify=temp_df['label'])\n",
    "\n",
    "# Custom Dataset class\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None, tokenizer=None):\n",
    "        self.data = dataframe\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.iloc[idx, 0]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.data.iloc[idx, 2]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        caption = self.data.iloc[idx, 1]\n",
    "        text_encoded = self.tokenizer(caption, return_tensors='pt', truncation=True, padding='max_length', max_length=128)\n",
    "        \n",
    "        for key in text_encoded:\n",
    "            text_encoded[key] = text_encoded[key].squeeze(0)\n",
    "        \n",
    "        return image, text_encoded, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Define image transformations\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.CenterCrop(224),     # Center crop to 1:1 aspect ratio (224x224)\n",
    "#     transforms.Resize((224, 224)),  # Resize to 224x224 (if needed to enforce size)\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "\n",
    "image_size = 224\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(image_size, scale=(0.7, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandAugment(num_ops=2, magnitude=9),  # torchvision>=0.13\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                         std=(0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize(int(image_size * 1.14)),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                         std=(0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "\n",
    "# Initialize DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Create Dataset instances for training, validation, and test sets\n",
    "train_dataset = MultiModalDataset(dataframe=train_df, transform=train_tf, tokenizer=tokenizer)\n",
    "val_dataset = MultiModalDataset(dataframe=val_df, transform=val_tf, tokenizer=tokenizer)\n",
    "test_dataset = MultiModalDataset(dataframe=test_df, transform=val_tf, tokenizer=tokenizer)\n",
    "\n",
    "# Create DataLoaders for each set\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=0, pin_memory=True, worker_init_fn=seed_worker, generator=g) \n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=0, pin_memory=True, worker_init_fn=seed_worker, generator=g)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=0, pin_memory=True, worker_init_fn=seed_worker, generator=g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30f0c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torchvision import models\n",
    "\n",
    "class LightweightMultiModalClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(LightweightMultiModalClassifier, self).__init__()\n",
    "        # Load pretrained SqueezeNet and freeze its parameters\n",
    "        self.squeezenet = models.squeezenet1_1(pretrained=True)\n",
    "        for param in self.squeezenet.parameters():\n",
    "            param.requires_grad = False  # Freeze SqueezeNet\n",
    "        \n",
    "        # Modify SqueezeNet's classifier to output 256 features\n",
    "        # The final output of SqueezeNet is 512 channels for its feature extractor\n",
    "        self.squeezenet.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Conv2d(512, 256, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # Load TinyBERT and freeze its parameters\n",
    "        self.bert = AutoModel.from_pretrained('prajjwal1/bert-tiny')\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False  # Freezes TinyBERT\n",
    "        \n",
    "        self.fc_text = nn.Linear(128, 256)  # TinyBERT has a hidden size of 128\n",
    "        \n",
    "        # Combined classifier with smaller fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        # Image processing with SqueezeNet\n",
    "        image_features = self.squeezenet(image)\n",
    "        \n",
    "        # Text processing with TinyBERT\n",
    "        text_features = self.bert(**text).last_hidden_state[:, 0, :]  # Using CLS token\n",
    "        text_features = self.fc_text(text_features)\n",
    "        \n",
    "        # Combine and classify\n",
    "        combined_features = torch.cat((image_features, text_features), dim=1)\n",
    "        output = self.fc(combined_features)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "530879a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TinyBERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('prajjwal1/bert-tiny')\n",
    "\n",
    "# Example tokenization\n",
    "text = \"Example caption text.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6abe8030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_param_groups(m, base_lr_text=3e-5, head_lr=1e-3, wd=0.01):\n",
    "    groups = []\n",
    "    for n, p in m.named_parameters():\n",
    "        if not p.requires_grad: \n",
    "            continue\n",
    "        # DistilBERT 쪽은 낮은 lr, 헤더/퓨전/클래시파이어는 높은 lr\n",
    "        lr = base_lr_text if (\"distilbert\" in n.lower()) else head_lr\n",
    "        # bias와 LayerNorm 계열은 weight decay 제외\n",
    "        if any(x in n for x in [\"bias\", \"LayerNorm.weight\", \"LayerNorm.bias\"]):\n",
    "            groups.append({\"params\":[p], \"lr\": lr, \"weight_decay\": 0.0})\n",
    "        else:\n",
    "            groups.append({\"params\":[p], \"lr\": lr, \"weight_decay\": wd})\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b9c5d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hj/anaconda3/envs/HJ/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/hj/anaconda3/envs/HJ/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SqueezeNet1_1_Weights.IMAGENET1K_V1`. You can also use `weights=SqueezeNet1_1_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Prepare the model, loss function, and optimizer\n",
    "num_classes = len(label_mapping)\n",
    "model = LightweightMultiModalClassifier(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "optimizer = optim.AdamW(build_param_groups(model), eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45bde3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the model\n",
    "def save_model(model, optimizer, epoch, path=\"lightweight_multimodal_model_HJ.pth\"):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, path)\n",
    "\n",
    "# Function to load the model\n",
    "def load_model(model, optimizer, path=\"lightweight_multimodal_model_HJ.pth\"):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    return model, optimizer, start_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40710d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:54<00:00,  2.25s/it]\n",
      "100%|██████████| 8/8 [00:11<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 0.8334, Val Loss: 0.5270, Val Accuracy: 88.80%\n",
      "Validation loss improved; model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:54<00:00,  2.27s/it]\n",
      "100%|██████████| 8/8 [00:11<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Train Loss: 0.5124, Val Loss: 0.4522, Val Accuracy: 91.65%\n",
      "Validation loss improved; model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:37<00:00,  1.56s/it]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Train Loss: 0.4643, Val Loss: 0.4104, Val Accuracy: 93.60%\n",
      "Validation loss improved; model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:36<00:00,  1.52s/it]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100], Train Loss: 0.4512, Val Loss: 0.3995, Val Accuracy: 93.45%\n",
      "Validation loss improved; model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:36<00:00,  1.52s/it]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100], Train Loss: 0.4329, Val Loss: 0.4080, Val Accuracy: 92.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:36<00:00,  1.52s/it]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100], Train Loss: 0.4239, Val Loss: 0.3937, Val Accuracy: 93.05%\n",
      "Validation loss improved; model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:36<00:00,  1.52s/it]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100], Train Loss: 0.4129, Val Loss: 0.4027, Val Accuracy: 92.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:36<00:00,  1.52s/it]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100], Train Loss: 0.4040, Val Loss: 0.3962, Val Accuracy: 92.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:36<00:00,  1.53s/it]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100], Train Loss: 0.4054, Val Loss: 0.3884, Val Accuracy: 93.30%\n",
      "Validation loss improved; model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:36<00:00,  1.53s/it]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Train Loss: 0.3962, Val Loss: 0.3827, Val Accuracy: 93.60%\n",
      "Validation loss improved; model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:36<00:00,  1.52s/it]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100], Train Loss: 0.3969, Val Loss: 0.3972, Val Accuracy: 92.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:36<00:00,  1.51s/it]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100], Train Loss: 0.3947, Val Loss: 0.3833, Val Accuracy: 93.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:36<00:00,  1.51s/it]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/100], Train Loss: 0.3875, Val Loss: 0.3792, Val Accuracy: 93.65%\n",
      "Validation loss improved; model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:36<00:00,  1.52s/it]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100], Train Loss: 0.3975, Val Loss: 0.3807, Val Accuracy: 93.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:36<00:00,  1.52s/it]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/100], Train Loss: 0.3846, Val Loss: 0.3850, Val Accuracy: 94.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:36<00:00,  1.52s/it]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/100], Train Loss: 0.3876, Val Loss: 0.4127, Val Accuracy: 93.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:36<00:00,  1.51s/it]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/100], Train Loss: 0.3820, Val Loss: 0.3756, Val Accuracy: 94.30%\n",
      "Validation loss improved; model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:35<00:00,  1.50s/it]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/100], Train Loss: 0.3743, Val Loss: 0.3771, Val Accuracy: 94.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:36<00:00,  1.52s/it]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/100], Train Loss: 0.3727, Val Loss: 0.3871, Val Accuracy: 94.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:36<00:00,  1.51s/it]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100], Train Loss: 0.3714, Val Loss: 0.3761, Val Accuracy: 93.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:36<00:00,  1.52s/it]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/100], Train Loss: 0.3598, Val Loss: 0.3790, Val Accuracy: 94.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:36<00:00,  1.51s/it]\n",
      "100%|██████████| 8/8 [00:08<00:00,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/100], Train Loss: 0.3660, Val Loss: 0.3832, Val Accuracy: 94.00%\n",
      "Early stopping at epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5  # Number of epochs to wait for improvement\n",
    "min_val_loss = np.Inf\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Training loop with early stopping\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, texts, labels in tqdm.tqdm(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        texts = {k: v.to(device) for k, v in texts.items()}\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, texts, labels in tqdm.tqdm(val_loader):  # Assuming val_loader is defined\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            texts = {k: v.to(device) for k, v in texts.items()}\n",
    "\n",
    "            outputs = model(images, texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    # Calculate average validation loss and accuracy\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = correct_predictions / total_samples\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "    \n",
    "    # Early stopping\n",
    "    if avg_val_loss < min_val_loss:\n",
    "        min_val_loss = avg_val_loss\n",
    "        epochs_no_improve = 0\n",
    "        save_model(model, optimizer, epoch)  # Save the best model\n",
    "        print(f\"Validation loss improved; model saved.\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31b2d537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from epoch 16\n"
     ]
    }
   ],
   "source": [
    "# Load the model and optimizer\n",
    "model, optimizer, start_epoch = load_model(model, optimizer, path=\"lightweight_multimodal_model_HJ.pth\")\n",
    "print(f\"Resuming from epoch {start_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c2328d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:08<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 93.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Variables to track the total loss and accuracy\n",
    "total_loss = 0.0\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "\n",
    "# Use no_grad context as we are not training, only evaluating\n",
    "with torch.no_grad():\n",
    "    for images, texts, labels in tqdm.tqdm(test_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        texts = {k: v.to(device) for k, v in texts.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images, texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "# Calculate average loss and accuracy\n",
    "average_loss = total_loss / len(test_loader)\n",
    "accuracy = correct_predictions / total_samples\n",
    "\n",
    "# print(f\"Test Loss: {average_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "453a292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Accuracy: 93.30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969d0191",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HJ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
