{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Clustering Analysis\n",
    "\n",
    "Sep 2019  \n",
    "Dongmin Kim\n",
    "[underkoo@kaist.ac.kr](mailto:underkoo@kaist.ac.kr)\n",
    "\n",
    "**Expected input data:**\n",
    "Process 900 dim latent feature of multi-modal instagram posts\n",
    "\n",
    "**What this script does:**  \n",
    "Explorative data analysis on the mult-modal latent. Checks for feature correlation (features = 900 dimesnsions), applies and visualized PCA, applies t-SNE on different data (raw, PCA, etc).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import copy\n",
    "import collections\n",
    "import seaborn as sns; sns.set()\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from MulticoreTSNE import MulticoreTSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import Birch, SpectralClustering, AffinityPropagation, AgglomerativeClustering, MiniBatchKMeans, KMeans, DBSCAN, OPTICS\n",
    "import matplotlib.colors as colors\n",
    "import math\n",
    "import time\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magic line to have in-notebook interactive plots\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you want to use a korean words, unblock it!\n",
    "# path_gothic = './fonts/NanumBarunGothic.ttf'\n",
    "# fontprop1 = fm.FontProperties(fname=path_gothic, size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = config.Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load latent feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw data\n",
    "target_csv = 'text_doc2vec_seoul_subway'\n",
    "df_data = pd.read_csv(os.path.join(CONFIG.CSV_PATH, target_csv + '.csv'), index_col=0, header=0, encoding='utf-8-sig')\n",
    "print('complete csv shape: ' + str(df_data.shape))\n",
    "print(df_data.iloc[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data seems to be well normalized to mean or median = 0 and std = 1\n",
    "plt.figure()\n",
    "df_data.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_normalized = df_data.div((np.sqrt(np.sum(np.square(df_data), axis=1))), axis=0)\n",
    "df_normalized.to_csv(os.path.join(CONFIG.CSV_PATH, 'normalized_' + target_csv + '.csv'), encoding='utf-8-sig')\n",
    "#df_normalized = pd.read_csv(os.path.join(CONFIG.CSV_PATH, 'normalized_' + target_csv + '.csv'), index_col=0, header=0, encoding='utf-8-sig')\n",
    "print('complete csv shape: ' + str(df_normalized.shape))\n",
    "print(df_normalized.iloc[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sqrt(np.sum(np.square(df_normalized), axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_normalized.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data seems to be well normalized to mean or median = 0 and std = 1\n",
    "plt.figure()\n",
    "df_normalized.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA as dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatterplot_pointlabels(df_twocols, markersize=None):\n",
    "    # basic scatterplot\n",
    "    fig = plt.figure()\n",
    "    plt.plot(df_twocols.iloc[:, 0], df_twocols.iloc[:, 1], marker='.', linestyle='None', markersize=markersize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca = PCA(n_components=100)\n",
    "df_pca_data = pd.DataFrame(pca.fit_transform(df_data))\n",
    "df_pca_data.columns = ['PC' + str(i) for i in range(df_pca_data.shape[1])]\n",
    "df_pca_data.index = df_data.index\n",
    "print('singular_values_:\\n' + str(pca.singular_values_))\n",
    "print('components_:\\n' + str(pca.components_))\n",
    "\n",
    "# create PCA transformation DF\n",
    "df_pca_transformation = pd.DataFrame(pca.components_).transpose()\n",
    "df_pca_transformation.columns = ['PC' + str(i) for i in range(1, pca.n_components_ + 1)]\n",
    "df_pca_transformation.index = df_data.columns\n",
    "#print(df_pca_transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA: plot variance explained\n",
    "plt.figure()\n",
    "plt.plot(pca.explained_variance_ratio_, marker='o', linestyle=':')\n",
    "plt.title('PCA: explained variance')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle=':')\n",
    "plt.title('PCA: cumulative explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot PC dimensions\n",
    "for dimen1 in range(0, 5):\n",
    "    for dimen2 in range(dimen1 + 1, 5):\n",
    "        # subset dataframe to the PC columns we need\n",
    "        df_tmp = df_pca_data.iloc[:, [dimen1, dimen2]]\n",
    "        \n",
    "        # plot without labels\n",
    "        scatterplot_pointlabels(df_tmp)\n",
    "        # plt.title(df_tmp.columns)\n",
    "        plt.xlabel(df_tmp.columns[0])\n",
    "        plt.ylabel(df_tmp.columns[1])\n",
    "        \n",
    "        # plot with labels\n",
    "        #scatterplot_pointlabels(df_tmp)\n",
    "        # plt.title(df_tmp.columns)\n",
    "        #plt.xlabel(df_tmp.columns[0])\n",
    "        #plt.ylabel(df_tmp.columns[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca_normalized = PCA(n_components=100)\n",
    "df_pca_normalized = pd.DataFrame(pca_normalized.fit_transform(df_normalized))\n",
    "df_pca_normalized.columns = ['PC' + str(i) for i in range(df_pca_normalized.shape[1])]\n",
    "df_pca_normalized.index = df_normalized.index\n",
    "print('singular_values_:\\n' + str(pca_normalized.singular_values_))\n",
    "print('components_:\\n' + str(pca_normalized.components_))\n",
    "\n",
    "# create PCA transformation DF\n",
    "df_pca_transformation_normalized = pd.DataFrame(pca_normalized.components_).transpose()\n",
    "df_pca_transformation_normalized.columns = ['PC' + str(i) for i in range(1, pca_normalized.n_components_ + 1)]\n",
    "df_pca_transformation_normalized.index = df_normalized.columns\n",
    "#print(df_pca_transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA: plot variance explained\n",
    "plt.figure()\n",
    "plt.plot(pca_normalized.explained_variance_ratio_, marker='o', linestyle=':')\n",
    "plt.title('PCA: explained variance')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca_normalized.explained_variance_ratio_), marker='o', linestyle=':')\n",
    "plt.title('PCA: cumulative explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot PC dimensions\n",
    "for dimen1 in range(0, 5):\n",
    "    for dimen2 in range(dimen1 + 1, 5):\n",
    "        # subset dataframe to the PC columns we need\n",
    "        df_tmp = df_pca_normalized.iloc[:, [dimen1, dimen2]]\n",
    "        \n",
    "        # plot without labels\n",
    "        scatterplot_pointlabels(df_tmp)\n",
    "        # plt.title(df_tmp.columns)\n",
    "        plt.xlabel(df_tmp.columns[0])\n",
    "        plt.ylabel(df_tmp.columns[1])\n",
    "        \n",
    "        # plot with labels\n",
    "        #scatterplot_pointlabels(df_tmp)\n",
    "        # plt.title(df_tmp.columns)\n",
    "        #plt.xlabel(df_tmp.columns[0])\n",
    "        #plt.ylabel(df_tmp.columns[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca_data.to_csv(os.path.join(CONFIG.CSV_PATH, 'pca_' + target_csv + '.csv'), encoding='utf-8-sig')\n",
    "#df_pca_data = pd.read_csv(os.path.join(CONFIG.CSV_PATH, 'pca_' + target_csv + '.csv'), index_col=0, header=0, encoding='utf-8-sig')\n",
    "df_pca_normalized.to_csv(os.path.join(CONFIG.CSV_PATH, 'pca_normalized_' + target_csv + '.csv'), encoding='utf-8-sig')\n",
    "#df_pca_data = pd.read_csv(os.path.join(CONFIG.CSV_PATH, 'pca_' + target_csv + '.csv'), index_col=0, header=0, encoding='utf-8-sig')\n",
    "print(df_pca_data.iloc[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsampling(df_data, num_samples=100):\n",
    "#     idx = int(len(df_data)/num_samples)\n",
    "#     idx_list = [x for x in range(0, len(df_data), idx)]\n",
    "#     return df_data.iloc[idx_list]\n",
    "    return df_data.sample(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_data = downsampling(df_data, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #ds_data.to_csv(os.path.join(CONFIG.CSV_PATH, 'ds_' + target_dataset + '.csv'), encoding='utf-8-sig')\n",
    "# ds_data = pd.read_csv(os.path.join(CONFIG.CSV_PATH, 'ds_' + target_dataset + '.csv'), index_col=0, header=0, encoding='utf-8-sig')\n",
    "# print(ds_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_tsne(tsne_object, data_to_pass):\n",
    "    data_tsne = pd.DataFrame(tsne_object.fit_transform(data_to_pass))\n",
    "    data_tsne.index = data_to_pass.index\n",
    "    data_tsne.columns = [['tsne1', 'tsne2']]\n",
    "    return data_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on pca\n",
    "# TODO tune the learning rate and the iterations (rest should be ok i guess)\n",
    "lr_list = [20, 100, 200]\n",
    "perp_list = [50, 200, 500, 1000]\n",
    "# lr_list = [200]\n",
    "# perp_list = [50]\n",
    "for lr in lr_list:\n",
    "    for perp in perp_list:     \n",
    "        start_time = time.time()\n",
    "        #tsne_pca = do_tsne(TSNE(n_components=2, perplexity=50, early_exaggeration=12.0, learning_rate=100, n_iter=5000, random_state=42, verbose=1), df_pca_data)\n",
    "        tsne_pca = do_tsne(MulticoreTSNE(n_components=2, perplexity=perp, learning_rate=lr, n_iter=2000, random_state=42, n_jobs=4, verbose=1), df_pca_normalized)\n",
    "        print(\"time elapsed: \" + str(time.time()-start_time) + \" lr: \" + str(lr) + \" perp: \" + str(perp))\n",
    "        #scatterplot_pointlabels(tsne_pca, 0.2)\n",
    "        #plt.title('t-SNE on PCA data lr: ' + str(lr) + ' perp: ' + str(perp) )\n",
    "        #plt.savefig(os.path.join(CONFIG.SVG_PATH, 'tsne_pca_' + target_csv + '_' + str(lr) + '_' + str(perp) + '.svg'))\n",
    "        tsne_pca.to_csv(os.path.join(CONFIG.CSV_PATH, 'tsne_pca_normalized_' + target_csv + '_' + str(lr) + '_' + str(perp) + '.csv'), encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot t-SNE with PCA\n",
    "\n",
    "for lr in lr_list:\n",
    "    for perp in perp_list:     \n",
    "        tsne_pca = pd.read_csv(os.path.join(CONFIG.CSV_PATH, 'tsne_pca_normalized_' + target_csv + '_' + str(lr) + '_' + str(perp) + '.csv'), index_col=0, header=0, encoding='utf-8-sig')\n",
    "        scatterplot_pointlabels(tsne_pca, 0.2)\n",
    "        plt.title('t-SNE on PCA data lr: ' + str(lr) + ' perp: ' + str(perp) )\n",
    "        plt.savefig(os.path.join(CONFIG.SVG_PATH, 'tsne_pca_' + target_csv + '_' + str(lr) + '_' + str(perp) + '.svg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tsne_pca.to_csv(os.path.join(CONFIG.CSV_PATH, 'tsne_' + target_dataset + '.csv'), encoding='utf-8-sig')\n",
    "#tsne_pca = pd.read_csv(os.path.join(CONFIG.CSV_PATH, 'tsne_' + target_csv + '.csv'), index_col=0, header=0, encoding='utf-8-sig')\n",
    "tsne_pca = pd.read_csv(os.path.join(CONFIG.CSV_PATH, 'tsne_pca_normalized_text_latent_instagram0830_200_50.csv'), index_col=0, header=0, encoding='utf-8-sig')\n",
    "tsne_pca = tsne_pca.iloc[1:]\n",
    "print(tsne_pca.iloc[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # on raw data\n",
    "# TODO tune the learning rate and the iterations (rest should be ok i guess)\n",
    "# start_time = time.time()\n",
    "# tsne_raw = do_tsne(MulticoreTSNE(n_components=2, perplexity=400, early_exaggeration=12.0, learning_rate=100, n_iter=10000, random_state=42, n_jobs=4), data)\n",
    "# print(\"time elapsed: \" + str(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot t-SNE with raw data\n",
    "# scatterplot_pointlabels(tsne_raw, plot_labels=False)\n",
    "# plt.title('t-SNE on raw data')\n",
    "# scatterplot_pointlabels(tsne_raw)\n",
    "# plt.title('t-SNE on raw data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp\n",
    "start_time = time.time()\n",
    "temp_clustering = KMeans(n_clusters = 24, random_state=42).fit(df_normalized)\n",
    "labels = temp_clustering.labels_\n",
    "print(\"time elapsed for clustering: \" + str(time.time()-start_time))\n",
    "start_time = time.time()\n",
    "print(silhouette_score(df_normalized, labels, metric = 'cosine'))\n",
    "print(\"time elapsed for silhouette_score: \" + str(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_score(X, max_num_clusters=10):\n",
    "    sse = []\n",
    "    sil = []\n",
    "    for k in range(2, max_num_clusters + 1):\n",
    "        print(\"current cluster: \" + str(k))\n",
    "        start_time = time.time()\n",
    "        temp_clustering = KMeans(n_clusters = k, random_state=42).fit(X)\n",
    "        print(\"time elapsed for clustering: \" + str(time.time()-start_time))\n",
    "        labels = temp_clustering.labels_\n",
    "        sse.append(temp_clustering.inertia_)\n",
    "        start_time = time.time()\n",
    "        sil.append(silhouette_score(X, labels))\n",
    "        print(\"time elapsed for silhouette_score: \" + str(time.time()-start_time))\n",
    "    return sse, sil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sse_distance(sse):\n",
    "    distance = []\n",
    "    last = len(sse) - 1\n",
    "    for i in range(1, last):\n",
    "        _distance = abs((sse[last] - sse[0]) * i - (last - 0) * sse[i] + last * sse[0] - sse[last] * 0) / math.sqrt(math.pow((sse[last] - sse[0]),2) + math.pow((last - 0),2))\n",
    "        distance.append(_distance)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_num_clusters = 30\n",
    "sse, sil = clustering_score(df_normalized, max_num_clusters)\n",
    "fig = plt.figure()\n",
    "plt.plot(range(2, max_num_clusters + 1), sse, marker='o')\n",
    "plt.xlabel('num of clusters')\n",
    "plt.ylabel('SSE')\n",
    "distance = sse_distance(sse)\n",
    "fig = plt.figure()\n",
    "plt.plot(range(3, max_num_clusters), distance, marker='o')\n",
    "plt.xlabel('num of clusters')\n",
    "plt.ylabel('SSE-distance')\n",
    "fig = plt.figure()\n",
    "plt.plot(range(2, max_num_clusters + 1), sil, marker='o')\n",
    "plt.xlabel('num of clusters')\n",
    "plt.ylabel('SIL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labeling points of tnse clusters\n",
    "def label_point(x, y, val, ax):\n",
    "    a = pd.concat({'x': x, 'y': y, 'val': val}, axis=1)\n",
    "    for i, point in a.iterrows():\n",
    "        ax.text(point['x']+.02, point['y'], str(point['val']), fontsize=3, fontproperties=fontprop1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_percentage(cluster_labels):\n",
    "    count = collections.Counter(cluster_labels)\n",
    "    for k in count:\n",
    "        print(\"cluster {} : {:.2%}\".format(str(k), count[k]/len(cluster_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_clustering(df, tsne):\n",
    "        start_time = time.time()\n",
    "        #clustering = Birch(n_clusters=21).fit(df_data)\n",
    "        #clustering = DBSCAN(eps=3, min_samples=5).fit(tsne_pca)\n",
    "        #clustering = SpectralClustering(n_clusters=num_clusters, random_state=42).fit(df)\n",
    "        #clustering = AgglomerativeClustering(n_clusters=num_clusters).fit(data.loc[filtered_columns.index,:])# data, df_pca_data\n",
    "        clustering = KMeans(n_clusters=num_clusters).fit(df_normalized)# data, df_pca_data\n",
    "        count_percentage(clustering.labels_)\n",
    "        print(clustering.labels_)\n",
    "        print(\"time elapsed: \" + str(time.time()-start_time))\n",
    "        dist = copy.deepcopy(tsne)\n",
    "        dist = dist.loc[df.index]\n",
    "        dist['cluster'] = clustering.labels_\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spectral clustering\n",
    "#For using 'amg' you should install pyamg in you local computer.\n",
    "# Check the component at\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html\n",
    "clustered_df = do_clustering(df_normalized, tsne_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(data=clustered_df['cluster'], index=clustered_df.index, columns=['cluster'])\n",
    "result_df.to_csv(os.path.join(CONFIG.CSV_PATH, 'clustered_kmeans_' + target_csv + '.csv'), encoding='utf-8-sig')\n",
    "#clustered_df = pd.read_csv(os.path.join(CONFIG.CSV_PATH, 'clustered_agglo_complete_normalized_text_latent_instagram0830.csv'), index_col=0, header=0, encoding='utf-8-sig')\n",
    "print(clustered_df.iloc[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_spectral_clustering(dataframe, markersize=None):\n",
    "    # plot spectrac clustering result\n",
    "    # without label\n",
    "    #scatterplot_pointlabels(dataframe[['x', 'y']], markersize)\n",
    "    # better colors\n",
    "    plt.figure()\n",
    "    color_dict = {v: k for v, k in enumerate(sns.color_palette(\"Paired\", num_clusters))}\n",
    "    sns_plot = sns.scatterplot(x=\"x\", y=\"y\", hue='cluster_id', palette=color_dict, data=dataframe, marker='.', s=markersize)\n",
    "    plt.savefig(os.path.join(CONFIG.SVG_PATH, 'clustering_' + target_csv + '.svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put clustering info into df\n",
    "df_clustered_spectral = pd.DataFrame({'x': tsne_pca.tsne1.squeeze(), \n",
    "                                      'y': tsne_pca.tsne2.squeeze(), \n",
    "                                      'cluster_id': clustered_df.cluster.squeeze()})\n",
    "# debug prints\n",
    "#print(df_clustered_spectral.dtypes)\n",
    "plot_spectral_clustering(df_clustered_spectral, 30)       \n",
    "#ds_clustered_spectral = downsampling(df_clustered_spectral, 5000)\n",
    "#plot_spectral_clustering(ds_clustered_spectral, 0.2)       \n",
    "# print(df_clustered_spectral.iloc[:10, :])\n",
    "# df_clustered_spectral['cluster_id'].describe() # 30 integer clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
